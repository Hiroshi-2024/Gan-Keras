*Maximum Classifier Discrepancy for Unsupervised Domain Adaptation*
(https://arxiv.org/pdf/1712.02560v1.pdf)

*Summary*
Authors propose a method for Unsupervised Domain Adaptation where the generator
is trained to translate images outside of the source distribution using task-specific
classifiers (discriminators). In their method they note that by using a discriminator
that discriminates between whether images come from the source domain or the target
domain this results in the generator producing images with ambigous features near
the class boundaries to fool the discriminator. This is not desirable if we want the
generator to produce samples that are suitable for classification or detection task.
To this end instead of using traditional discriminators they introduce two task classifiers
F1 and F2 that are fed the features of the data extracted by the generator G. The idea is that
the features extracted from the source domain can be classified with a high accuracy due to
there being a lot of annotated samples available, while the performance on the target domain
will not be sufficiently high for a classifier trained on the source domain. To detect these
target samples the authors propose to utilize the disagreement of the two classifiers on the
sample. The objective of the discriminators is to maximize this disagreement due to the samples
being from different domains. Conversely, the generator is trained to minimize
the disageement. By minimizing the disagreement the generator will avoid generating target
features outside the support of the source.

They also demonstrate that F1 and F2 could be assigned detection or segmentation tasks as well.

*Dicrepancy Loss*
As their discrepancy loss they apply a L1 loss between the softmax output of the
two classifiers F1 and F2.


*Training Steps*
These steps are repeated.
Step A: In this step they train the classifiers to minimize the classification cross entropy
loss on the source data.
Step B: They train the classifiers as a discriminator for a fixed (freezed weights) G.
Their objective is to maximize the discrepancy loss.
Step C: They weights of the classifiers are freezed and the generator is trained to minimize
the discrepancy loss.
